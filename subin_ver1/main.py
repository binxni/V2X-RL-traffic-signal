import csv
import os
import datetime
import torch
import numpy as np
import traci
from torch.utils.tensorboard import SummaryWriter
from utils import save_hyperparams_to_csv
from env_sumo import SumoEnvironment
from ppo_agent import PPOAgent
from config import *
from env_sumo import get_average_waiting_time

def main():
    # --- í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ---
    env = SumoEnvironment(
        sumo_cfg_path=SUMO_CONFIG_FILE,  # SUMO ì‹œë®¬ë ˆì´ì…˜ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        max_steps=T_HORIZON,  # í•œ ì—í”¼ì†Œë“œì—ì„œ ìµœëŒ€ ì‹œë®¬ë ˆì´ì…˜ ìŠ¤í… ìˆ˜
        gui=USE_GUI ) # GUI ì‹¤í–‰ ì—¬ë¶€                  

    agent = PPOAgent(
        state_dim=STATE_DIM,
        action_dim=ACTION_DIM,
        hidden_dim=HIDDEN_DIM,
        lr=LR,
        gamma=GAMMA,
        clip_eps=EPS_CLIP,
        epochs=K_EPOCH
    )

    # --- TensorBoard ë° CSV ë¡œê·¸ íŒŒì¼ ì„¤ì • ---
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")
    writer = SummaryWriter(log_dir=f"runs/ppo_sumo_{timestamp}")

    os.makedirs("logs", exist_ok=True)
    csv_path = f"logs/train_log_{timestamp}.csv"
    with open(csv_path, "w", newline="") as f:
        writer_csv = csv.writer(f)
        writer_csv.writerow(
            ["Episode", "Reward", "PolicyLoss", "ValueLoss", "AvgWaitingTime"])

    os.makedirs("models_backup", exist_ok=True)
    best_models = []
    save_hyperparams_to_csv(config, entropy_coeff=0.2) 

    # --- í•™ìŠµ ë£¨í”„ ì‹œì‘ ---
    for episode in range(1, MAX_EPISODES + 1):
        # state initalize and observation
        state = env.reset()
        score = 0  # ëˆ„ì  ë¦¬ì›Œë“œ
        total_waiting_time = 0
        total_vehicle_count = 0
        avg_waiting_time_list = []
        for t in range(T_HORIZON):

            # í˜„ì¬ ìƒíƒœì—ì„œ í–‰ë™ ì„ íƒ
            action, log_prob = agent.select_action(state)

            # í™˜ê²½ì— ì•¡ì…˜ ì ìš©
            next_state, reward, done, _ = env.step(np.array(action))
            score += reward

            avg_waiting_time_list.append(get_average_waiting_time())

            # Transition ì €ì¥
            agent.store_transition(
                state, action, log_prob, reward, next_state, done)

            state = next_state
            
            # stepë§ˆë‹¤ agentì—ê²Œ ì•Œë ¤ì£¼ê¸°
            agent.total_steps += 1

            if done:
                break

        # --- PPO ì—ì´ì „íŠ¸ ì—…ë°ì´íŠ¸ ---
        policy_loss, value_loss = agent.update(next_state, done)

        avg_waiting_time = np.mean(avg_waiting_time_list)

        # --- ì½˜ì†” ì¶œë ¥ ---
        print(f"[Episode {episode:03d}] Reward: {score:.2f} | "
              f"Policy Loss: {policy_loss:.4f} | Value Loss: {value_loss:.4f} | "
              f"Avg Waiting Time: {avg_waiting_time:.2f}s")

        # --- TensorBoard ê¸°ë¡ ---
        writer.add_scalar("Reward/Total", score, episode)
        writer.add_scalar("Loss/Policy", policy_loss, episode)
        writer.add_scalar("Loss/Value", value_loss, episode)
        writer.add_scalar("Traffic/Average_Waiting_Time",
                          avg_waiting_time, episode)

        # --- CSV ê¸°ë¡ ---
        with open(csv_path, "a", newline="") as f:
            writer_csv = csv.writer(f)
            writer_csv.writerow(
                [episode, score, policy_loss, value_loss, avg_waiting_time])
            
        # --- ì—í”¼ì†Œë“œë³„ ëª¨ë¸ ì €ì¥ ---
        backup_path = f"models_backup/ppo_sumo_ep{episode:03d}_{timestamp}.pth"
        torch.save({
            'policy': agent.policy_net.state_dict(),
            'value': agent.value_net.state_dict(),
            'optimizer_policy': agent.optimizer_policy.state_dict(),
            'optimizer_value': agent.optimizer_value.state_dict(),
            'avg_waiting_time': avg_waiting_time,
            'episode': episode
        }, backup_path)

        # best_models ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        best_models.append((avg_waiting_time, backup_path))

    # --- ìµœì¢… ëª¨ë¸ ì €ì¥ ---
    torch.save({
        'policy': agent.policy_net.state_dict(),
        'value': agent.value_net.state_dict(),
        'optimizer_policy': agent.optimizer_policy.state_dict(),
        'optimizer_value': agent.optimizer_value.state_dict(),
        'avg_waiting_time': avg_waiting_time,
        'episode': episode
    }, backup_path)

    writer.close()
    print("\nâœ… Training Complete. Model saved to:", MODEL_SAVE_PATH)
    # ì €ì¥ëœ checkpoint ë¡œë“œ

    best_model = min(best_models, key=lambda x: x[0])  # (avg_waiting_time, path)
    best_avg_waiting_time, best_model_path = best_model

    # --- í•´ë‹¹ ëª¨ë¸ ë¡œë“œ ë° í‰ê·  ëŒ€ê¸°ì‹œê°„ ì¶œë ¥ ---
    checkpoint = torch.load(best_model_path, map_location=torch.device('cpu'), weights_only=False)
    print(f"\nğŸ† Best Model: {best_model_path}")
    print("Best avg_waiting_time:", checkpoint['avg_waiting_time'])
    print("Best episode:", checkpoint['episode'])

if __name__ == '__main__':
    main()

# ëë‚˜ê³  ì•„ë˜ ëª…ë ¹ì–´ë¡œ tensorboard ì‹¤í–‰
# tensorboard --logdir=runs
# í•´ë‹¹ ëª…ë ¹ì–´ ì¹˜ê³ ë‚˜ì„œ ë¸Œë¼ìš°ì €ì— http://localhost:6006 í•´ì„œ ë“¤ì–´ê°€ë©´ ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸ ê°€ëŠ¥