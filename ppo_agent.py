import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import namedtuple
from collections import Counter
from torch.utils.tensorboard import SummaryWriter

# ê²½í—˜ ì €ì¥ìš© íŠœí”Œ
Transition = namedtuple(
    "Transition", ["state", "action", "log_prob", "reward", "next_state", "done"])

# ì‹ ê²½ë§: ì •ì±… (Actor)


class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, hidden_dim):
        super(PolicyNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim, 5)  # phase4ê°œì™€ duration 1ê°œ
        )

    def forward(self, x):
        return self.fc(x)

# ì‹ ê²½ë§: ê°€ì¹˜ í•¨ìˆ˜ (Critic)


class ValueNetwork(nn.Module):
    def __init__(self, state_dim, hidden_dim):
        super(ValueNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, x):
        return self.fc(x)


class PPOAgent:
    def __init__(self, state_dim, action_dim, hidden_dim, gamma=0.99, lr=3e-4, clip_eps=0.2, epochs=10):
        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.policy_net = PolicyNetwork(state_dim, hidden_dim)
        self.value_net = ValueNetwork(state_dim, hidden_dim)
        self.optimizer_policy = optim.Adam(self.policy_net.parameters(), lr=lr)
        self.optimizer_value = optim.Adam(self.value_net.parameters(), lr=lr)

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
        self.gamma = gamma
        self.clip_eps = clip_eps
        self.epochs = epochs

        # phase ë¶„í¬ ê¸°ë¡ìš© ë¡œê·¸ ì„¤ì • ì¶”ê°€
        self.phase_counter = Counter()
        self.writer = SummaryWriter(log_dir="runs/phase_distribution")
        self.memory = []  # Transition ì €ì¥ ë¦¬ìŠ¤íŠ¸
        self.total_steps = 0

    def select_action(self, state):
        state_tensor = torch.FloatTensor(state).unsqueeze(0)

        # ì •ì±… ë„¤íŠ¸ì›Œí¬ì˜ ì¶œë ¥ (4ê°œ phaseì— ëŒ€í•œ logit, duration í•˜ë‚˜)
        output = self.policy_net(state_tensor)
        phase_logits = output[:, :4]      # shape: [1, 4]
        duration_raw = output[:, 4]       # shape: [1]

        # Phase: Categorical ë¶„í¬ì—ì„œ ìƒ˜í”Œë§
        phase_probs = torch.softmax(phase_logits, dim=-1)
        phase_dist = torch.distributions.Categorical(phase_probs)
        phase = phase_dist.sample().item()
        log_prob = phase_dist.log_prob(torch.tensor(phase))

        # âœ… phase ì¹´ìš´íŠ¸ ì¶”ê°€
        self.phase_counter[phase] += 1

        # Duration: ì •ê·œ ë¶„í¬ ê¸°ë°˜ ìƒ˜í”Œë§ (í˜¹ì€ tanh ë“± í™œìš©)
        # duration = int(np.clip(duration_raw.item(), 5, 60))
        duration = int(5 + 55 * torch.sigmoid(duration_raw).item())

        return np.array([phase, duration]), log_prob

    def store_transition(self, *args):
        self.memory.append(Transition(*args))

    def compute_gae(self, next_state, done):
        # GAE ê³„ì‚° í•¨ìˆ˜
        values = self.value_net(torch.FloatTensor(
            [t.state for t in self.memory])).detach().squeeze()
        next_value = self.value_net(torch.FloatTensor(
            next_state)).detach() if not done else torch.tensor(0.0)
        rewards = [t.reward for t in self.memory]
        masks = [0.0 if t.done else 1.0 for t in self.memory]

        values = torch.cat([values, next_value.unsqueeze(0)])
        gae = 0
        returns = []

        LAMBDA = 0.95
        for step in reversed(range(len(rewards))):
            delta = rewards[step] + self.gamma * \
                values[step + 1] * masks[step] - values[step]
            gae = delta + self.gamma * LAMBDA * masks[step] * gae
            returns.insert(0, gae + values[step])

        return returns, [r - v.item() for r, v in zip(returns, values[:-1])]

    def update(self, next_state, done):
        # ì •ì±… ë° ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (PPO í•µì‹¬)
        transitions = self.memory
        returns, advantages = self.compute_gae(next_state, done)

        states = torch.FloatTensor([t.state for t in transitions])
        actions = torch.FloatTensor([t.action for t in transitions])
        old_log_probs = torch.stack([t.log_prob for t in transitions]).detach()
        returns = torch.FloatTensor(returns).unsqueeze(1)
        advantages = torch.FloatTensor(advantages).unsqueeze(1)

        for _ in range(self.epochs):
            # âœ… ì •ì±… ë„¤íŠ¸ì›Œí¬ì—ì„œ [phase_logits, duration_pred] ë¶„ë¦¬
            policy_output = self.policy_net(states)  # shape: [batch, 2]
            phase_logits = policy_output[:, 0:1]     # Phaseì— ëŒ€í•œ logits
            duration_pred = policy_output[:, 1]      # Duration ì˜ˆì¸¡ê°’ (í˜„ì¬ ì‚¬ìš© ì•ˆí•¨)

            # âœ… Categorical ë¶„í¬ì—ì„œ ìƒˆ log_prob ê³„ì‚° (phase)
            phase_probs = torch.softmax(phase_logits.squeeze(1), dim=0)
            dist = torch.distributions.Categorical(probs=phase_probs)
            new_log_probs = dist.log_prob(actions[:, 0].long()).unsqueeze(1)

            # âœ… PPO ratio ê³„ì‚°
            ratio = torch.exp(new_log_probs - old_log_probs)

            # âœ… Clipped surrogate objective
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1.0 - self.clip_eps,
                                1.0 + self.clip_eps) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()

            # âœ… ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ ì¶”ê°€
            entropy = dist.entropy().mean()  # í‰ê·  ì—”íŠ¸ë¡œí”¼
            entropy_coeff = 0.05  # íƒí—˜ ë¹„ì¤‘ ì¡°ì ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ê°’ì´ í¬ë©´ íƒí—˜ ì¦ê°€)
            policy_loss = policy_loss - entropy_coeff * entropy  # ì—”íŠ¸ë¡œí”¼ë¥¼ ë³´ë„ˆìŠ¤ë¡œ ì¶”ê°€

            # âœ… Value lossëŠ” ê·¸ëŒ€ë¡œ
            values = self.value_net(states)
            value_loss = nn.MSELoss()(values, returns)

            # ğŸ”„ ìµœì í™”
            self.optimizer_value.zero_grad()
            value_loss.backward()
            self.optimizer_value.step()

            self.optimizer_policy.zero_grad()
            policy_loss.backward()
            self.optimizer_policy.step()

            # âœ… phase ì„ íƒ ë¶„í¬ ê¸°ë¡
            total = sum(self.phase_counter.values())
            if total > 0:
                for p in range(4):  # phase 0~3
                    ratio = self.phase_counter[p] / total
                    # or self.episode if ìˆìŒ
                    self.writer.add_scalar("Phase/Selection_Ratio",
                                           ratio, self.total_steps)
            self.phase_counter.clear()  # ì—í”¼ì†Œë“œë³„ ì´ˆê¸°í™”

        self.memory = []
        return policy_loss.item(), value_loss.item()
